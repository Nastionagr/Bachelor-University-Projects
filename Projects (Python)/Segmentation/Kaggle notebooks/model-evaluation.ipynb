{"cells":[{"cell_type":"markdown","metadata":{},"source":["# EVALUATION OF THE CHOOSEN MODELS"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2022-05-29T12:09:07.674961Z","iopub.status.busy":"2022-05-29T12:09:07.674119Z","iopub.status.idle":"2022-05-29T12:09:07.680729Z","shell.execute_reply":"2022-05-29T12:09:07.679636Z","shell.execute_reply.started":"2022-05-29T12:09:07.674922Z"},"trusted":true},"outputs":[],"source":["# root_path       - path to the wound dataset\n","# csv_path_folder - path to the folder containing csvs with lists of image names used in given dataset (test, train, validate)\n","# unet_relu_path  - path to the defined model in .py file. Must be set if given model is being used\n","# unet_sigm_path  - path to the defined model in .py file. Must be set if given model is being used\n","# weights_xxx     - path to the file with pretrained weights of given model xxx (DeepLabV3, Unet-Sigmoid, Unet-ReLU)\n","\n","root_path =         '../input/350pics/dataset'\n","csv_path_folder =   '../input/wound-dataset-splitedlist/sort whole/'\n","\n","unet_relu_path =    '../input/sigmoid-model/unet_model.py'\n","unet_sigm_path =    '../input/nn-model/unet_model.py'\n","\n","weights_DeepLabV3 = '../input/model-weights/model_weights/deelpabv3_weights.pth'\n","weights_Unet_ReLU = '../input/model-weights/model_weights_new/unetRelu_weights.pth'\n","weights_Unet_Sigm = '../input/model-weights/model_weights_new/unetSigmoid_weights.pth'"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2022-05-29T12:09:07.778247Z","iopub.status.busy":"2022-05-29T12:09:07.777534Z","iopub.status.idle":"2022-05-29T12:09:07.783701Z","shell.execute_reply":"2022-05-29T12:09:07.782910Z","shell.execute_reply.started":"2022-05-29T12:09:07.778214Z"},"trusted":true},"outputs":[],"source":["import os\n","import csv\n","import time\n","import torch\n","import random\n","import numpy as np\n","import pandas as pd\n","import seaborn as sb\n","import torch.nn as nn\n","from skimage import io\n","import torch.utils.data as data\n","import matplotlib.pyplot as plt\n","from torchvision import transforms#, datasets\n","from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, accuracy_score, classification_report, matthews_corrcoef\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2022-05-29T12:09:07.879783Z","iopub.status.busy":"2022-05-29T12:09:07.879411Z","iopub.status.idle":"2022-05-29T12:09:07.887710Z","shell.execute_reply":"2022-05-29T12:09:07.886992Z","shell.execute_reply.started":"2022-05-29T12:09:07.879756Z"},"trusted":true},"outputs":[],"source":["# Calling these functions fill download and set up given models. If a path to model weights is included, weights will be automaticaly loaded\n","\n","def createUnet(weights_path=None, model_type='ReLU'):\n","    assert model_type in ('ReLU', 'Sigmoid'), \"Unknown Unet model type\"\n","    # Loading model from the python file\n","    from shutil import copyfile\n","    unetpath = unet_relu_path if model_type == 'Unet-Sigmoid' else unet_sigm_path\n","    copyfile(src = unetpath, dst = \"../working/unet_model.py\")\n","    from unet_model import Unet\n","    model = Unet()\n","    \n","    if weights_path is not None:\n","        model.load_state_dict(torch.load(weights_path, map_location=device))\n","        \n","    model = model.to(device)\n","    return model\n","    \n","    \n","def createDeepLabV3(weights_path=None):\n","    from torchvision.models.segmentation.deeplabv3 import DeepLabHead\n","    from torchvision import models\n","    model = models.segmentation.deeplabv3_resnet101(pretrained=True, progress=True)\n","    outputchannels=4\n","    model.classifier = DeepLabHead(2048, outputchannels)\n","    if weights_path is not None:\n","        model.load_state_dict(torch.load(weights_path, map_location=device))\n","    \n","    model = model.to(device)\n","    return model"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2022-05-29T12:09:07.975001Z","iopub.status.busy":"2022-05-29T12:09:07.974559Z","iopub.status.idle":"2022-05-29T12:09:07.985189Z","shell.execute_reply":"2022-05-29T12:09:07.983943Z","shell.execute_reply.started":"2022-05-29T12:09:07.974974Z"},"trusted":true},"outputs":[],"source":["# Creating the class to work with the dataset\n","class WoundDataset(data.Dataset):\n","\n","    def __init__(self, root, transform=None, csv_file=None):\n","        self.img_orig = root + '/imgs'  # folder with the resized images (512 * 512)\n","        self.img_mask = root + '/masks' # folder with the masks to these images\n","        self.transform = transform\n","        \n","        if csv_file is None:\n","            self.imglist = os.listdir(self.img_orig)\n","        else:\n","            with open(csv_file, newline='') as csvfile:\n","                spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n","                csv_img_list = [row[1] for row in spamreader][1:]\n","                self.imglist = [filename for filename in os.listdir(self.img_orig) if filename in csv_img_list]\n","                \n","    def __len__(self):\n","        return len(self.imglist)\n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","\n","        img_name = os.path.join(self.img_orig, self.imglist[idx])\n","        msk_name = os.path.join(self.img_mask, self.imglist[idx])\n","        \n","        img = io.imread(img_name)\n","        msk = io.imread(msk_name)[:,:,:3] # reading just RGB channels (without hue)\n","        \n","        if self.transform:\n","            img = self.transform(img)\n","            msk = self.transform(msk)\n","\n","        return img, msk"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2022-05-29T12:09:08.169588Z","iopub.status.busy":"2022-05-29T12:09:08.169092Z","iopub.status.idle":"2022-05-29T12:09:08.181377Z","shell.execute_reply":"2022-05-29T12:09:08.180592Z","shell.execute_reply.started":"2022-05-29T12:09:08.169540Z"},"trusted":true},"outputs":[],"source":["# Function for coding the input to the NN\n","\n","def convert_into_1d(old_tensor):\n","    # pixels that will be marked as a backgroung\n","    max_pixel_value = (torch.max(torch.flatten(old_tensor))).cpu().numpy()\n","    color_threshold = 0.5 * max_pixel_value\n","    \n","    # 1 - additional channel for the class labeling\n","    new_tensor = torch.full((old_tensor.shape[0], 1, old_tensor.shape[2], old_tensor.shape[3]), color_threshold)\n","    \n","    old_tensor = old_tensor.to(device)\n","    new_tensor = new_tensor.to(device)\n","    \n","    # concatenate two tensors\n","    old_tensor = torch.cat((old_tensor, new_tensor), dim=1)\n","    \n","    # finding the layer with the maximum pixel value\n","    max_idxs = torch.argmax(old_tensor, dim=1)\n","                    \n","    return max_idxs\n","\n","\n","def converting_to_rgb_layers(x):\n","    switcher = {\n","        0: [255, 0, 0], # granulation tissue\n","        1: [0, 255, 0], # slough tissue\n","        2: [0, 0, 255], # necrotic tissue\n","        3: [0, 0, 0]    # background\n","    }\n","    return switcher.get(int(x[0]), \"error\")\n","\n","\n","# Function for decoding final mask (NN output)\n","def convert_into_3d(output_mask):\n","    # create a new 'empty' tensor\n","    zero_tensor = torch.zeros(output_mask.shape[1], output_mask.shape[2], 1)\n","    \n","    # detecting which layer has to be displayed in the picture\n","    dominant_layers = torch.argmax(torch.tensor(output_mask), dim=0).unsqueeze(2)\n","    \n","    # creating a new 3-dimentional tensor\n","    # first dim - dominant layer, other - empty layers\n","    three_dim_tensor = torch.cat((dominant_layers, zero_tensor, zero_tensor), dim=2)\n","    # converting it to numpy array\n","    numpy_array = three_dim_tensor.detach().numpy()\n","        \n","    # changing dominant layers to rgb image\n","    numpy_array = np.apply_along_axis(converting_to_rgb_layers, -1, numpy_array)\n","        \n","    return numpy_array"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2022-05-29T12:09:08.276558Z","iopub.status.busy":"2022-05-29T12:09:08.276205Z","iopub.status.idle":"2022-05-29T12:09:08.283605Z","shell.execute_reply":"2022-05-29T12:09:08.281631Z","shell.execute_reply.started":"2022-05-29T12:09:08.276531Z"},"trusted":true},"outputs":[],"source":["# Function that helps to display original image, binary truly mask to it, NN generated output mask and the difference between the last two\n","\n","def display_binary_masks(index, pictures):\n","    fig, axs = plt.subplots(1, 6, figsize=(24,4))\n","    for i, picture in enumerate(pictures):\n","        axs[i].imshow(np.transpose(picture.cpu().numpy(), (1, 2, 0)), cmap = 'gray' if i in (2, 4, 5) else None)\n","    fig.suptitle('Processing image number ' + str(index + 1), fontweight ='bold')    \n","    plt.show()   "]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2022-05-29T12:09:08.369514Z","iopub.status.busy":"2022-05-29T12:09:08.369253Z","iopub.status.idle":"2022-05-29T12:09:08.373609Z","shell.execute_reply":"2022-05-29T12:09:08.372869Z","shell.execute_reply.started":"2022-05-29T12:09:08.369490Z"},"trusted":true},"outputs":[],"source":["# Function that helps to convert all classes (r, g, b) to white color \n","# Function that helps to create a binary mask\n","\n","def binarise_tensor(tensor):\n","    max_color_layer, max_ids = torch.max(tensor, 0)\n","    return max_color_layer.unsqueeze(0)"]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2022-05-29T12:20:44.961199Z","iopub.status.busy":"2022-05-29T12:20:44.960676Z","iopub.status.idle":"2022-05-29T12:20:44.969413Z","shell.execute_reply":"2022-05-29T12:20:44.968537Z","shell.execute_reply.started":"2022-05-29T12:20:44.961163Z"},"trusted":true},"outputs":[],"source":["# Function that helps to display the confusion matrix\n","\n","def display_confusion_matrix(truth, prediction, save_name=None):\n","    cm = confusion_matrix(truth, prediction, labels = [0, 1, 2, 3], normalize = 'true')\n","    cm_df = pd.DataFrame(cm, \n","                         index = ['GRANULATION TISSUE','SLOUGH TISSUE','NECROTIC TISSUE', 'BACKGROUND'],\n","                         columns = ['GRANULATION TISSUE','SLOUGH TISSUE','NECROTIC TISSUE', 'BACKGROUND'])\n","    fig = plt.figure(figsize=(7.5,7.5 ))\n","    sb.heatmap(cm_df, annot=True, fmt='.2%')\n","    plt.title('Confusion Matrix')\n","    plt.ylabel('Actal Values')\n","    plt.xlabel('Predicted Values')\n","    \n","    if save_name is not None:\n","        plt.savefig(save_name)\n","    plt.show()"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2022-05-29T12:09:08.579172Z","iopub.status.busy":"2022-05-29T12:09:08.578698Z","iopub.status.idle":"2022-05-29T12:09:08.584910Z","shell.execute_reply":"2022-05-29T12:09:08.584093Z","shell.execute_reply.started":"2022-05-29T12:09:08.579147Z"},"trusted":true},"outputs":[],"source":["# Function that helps to calculate accuracy of the selected model\n","\n","def calculate_accuracy(index, image, mask, output_mask, display=False):\n","    binary_mask = binarise_tensor(mask)\n","    binary_output_mask = binarise_tensor(output_mask)\n","    difference = torch.logical_xor(binary_mask, binary_output_mask)\n","\n","    difference = torch.where(difference == True, 1, 0)\n","    num_diff = torch.sum(difference)\n","    current_accuracy = 1 - num_diff / (mask.shape[1] * mask.shape[2])\n","    \n","    if display:\n","        display_binary_masks(index, (image, mask, binary_mask, output_mask, binary_output_mask, difference))\n","        \n","    return current_accuracy.detach().cpu().numpy()\n"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2022-05-29T12:09:08.678250Z","iopub.status.busy":"2022-05-29T12:09:08.677913Z","iopub.status.idle":"2022-05-29T12:09:08.684555Z","shell.execute_reply":"2022-05-29T12:09:08.683770Z","shell.execute_reply.started":"2022-05-29T12:09:08.678224Z"},"trusted":true},"outputs":[],"source":["# Function evaluating score such as Dice score, Iterset-over-Union (IoU) and Matthews correlation coefficient (MCC)\n","def evaluate_mcc_iou_dice(gt, pred):        \n","    intersection = np.sum(np.where(np.logical_and(pred==gt, gt<3), 1, 0))\n","    union = np.sum(np.where(np.logical_or(pred<3, gt<3), 1, 0)) # 3 is the background\n","    area_pred = np.sum(np.where(pred<3, 1, 0))\n","    area_gt   = np.sum(np.where(gt  <3, 1, 0))\n","    ##############3\n","    \n","    mcc = matthews_corrcoef(gt, pred)\n","    iou = intersection / union\n","    dice= 2*intersection / (area_pred + area_gt)\n","    \n","    return [mcc, iou, dice, intersection, union, area_pred, area_gt]"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2022-05-29T12:09:08.768924Z","iopub.status.busy":"2022-05-29T12:09:08.768356Z","iopub.status.idle":"2022-05-29T12:09:08.780176Z","shell.execute_reply":"2022-05-29T12:09:08.779444Z","shell.execute_reply.started":"2022-05-29T12:09:08.768897Z"},"trusted":true},"outputs":[],"source":["# Main function for model performance testing\n","\n","def evaluate_and_test_model(model, data_test, display=False):\n","    accuracy = []\n","    times = []\n","    truth = []\n","    prediction = []\n","    \n","    eval_list = []\n","    true_mask_arr = np.array([])\n","    pred_mask_arr = np.array([])\n","    \n","    model.eval()\n","    \n","    for index in range(len(data_test)):\n","        image, mask = data_test[index]\n","        \n","        image = image.to(device)\n","        start_time = time.time()\n","        output_mask = model(image.unsqueeze(0))\n","        if isinstance(output_mask, dict):\n","            output_mask = output_mask['out']\n","        output_mask = output_mask.squeeze(0).cpu()\n","        elapsed_time = time.time() - start_time\n","        output_mask = torch.from_numpy(np.transpose(convert_into_3d(output_mask), (2,0,1)))\n","        \n","        current_accuracy = calculate_accuracy(index, image, mask, output_mask, display)  \n","#         evaluate_AccPreRecSens(index, image, mask, output_mask) \n","        true_mask = torch.flatten(convert_into_1d(mask.unsqueeze(0))).cpu().numpy()\n","        pred_mask = torch.flatten(convert_into_1d(output_mask.unsqueeze(0))).cpu().numpy()\n","\n","        accuracy.append(current_accuracy)\n","        times.append(elapsed_time)\n","        truth = [*truth, *true_mask]\n","        prediction = [*prediction, *pred_mask] \n","        \n","        eval_list.append(evaluate_mcc_iou_dice(true_mask, pred_mask))\n","        # mcc, iou, dice, intersection, union, area_pred, area_gt\n","        \n","    eval_list = np.array(eval_list)\n","\n","    return accuracy, times, truth, prediction, eval_list"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2022-05-29T12:09:08.872558Z","iopub.status.busy":"2022-05-29T12:09:08.872051Z","iopub.status.idle":"2022-05-29T12:09:08.881299Z","shell.execute_reply":"2022-05-29T12:09:08.880364Z","shell.execute_reply.started":"2022-05-29T12:09:08.872527Z"},"trusted":true},"outputs":[],"source":["# Funciton displaying majority of results\n","def evaluate(eval_list, truth, prediction):\n","    print(\"mcc  avg: {:.3f}\".format(np.mean(eval_list[:,0])))\n","    print(\"iou  avg: {:.3f}\".format(np.mean(eval_list[:,1])))\n","    print(\"dice avg: {:.3f}\".format(np.mean(eval_list[:,2])))\n","    \n","    print()\n","    print(\"mcc  median: {:.3f}\".format(np.median(eval_list[:,0])))\n","    print(\"iou  median: {:.3f}\".format(np.median(eval_list[:,1])))\n","    print(\"dice median: {:.3f}\".format(np.median(eval_list[:,2])))\n","    \n","    print()\n","    print(\"mcc  whole: {:.3f}\".format(matthews_corrcoef(truth, prediction)))\n","    print(\"iou  whole: {:.3f}\".format(np.sum(eval_list[:,3]) / np.sum(eval_list[:,4  ])))\n","    print(\"dice whole: {:.3f}\".format(2*np.sum(eval_list[:,3]) / np.sum(eval_list[:,4:6])))\n","    \n","    \n","    print()\n","    print(\"classification_report:\\n\",\n","          classification_report(\n","              truth, prediction, digits=3, labels=[0,1,2,3],\n","              target_names=['granulation tissue',\n","                            'slough tissue',\n","                            'necrotic tissue',\n","                            'background'\n","                           ], ))"]},{"cell_type":"markdown","metadata":{},"source":["# Evaluation"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2022-05-29T12:09:08.972419Z","iopub.status.busy":"2022-05-29T12:09:08.972155Z","iopub.status.idle":"2022-05-29T12:09:08.978219Z","shell.execute_reply":"2022-05-29T12:09:08.977469Z","shell.execute_reply.started":"2022-05-29T12:09:08.972396Z"},"trusted":true},"outputs":[],"source":["device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","print(f'Selected device: {device}')"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2022-05-29T12:09:09.071499Z","iopub.status.busy":"2022-05-29T12:09:09.070851Z","iopub.status.idle":"2022-05-29T12:09:09.080178Z","shell.execute_reply":"2022-05-29T12:09:09.079392Z","shell.execute_reply.started":"2022-05-29T12:09:09.071461Z"},"trusted":true},"outputs":[],"source":["data_test =  WoundDataset(root = root_path, transform = transforms.ToTensor(), csv_file=csv_path_folder+'test.csv')"]},{"cell_type":"markdown","metadata":{},"source":["# DeepLabV3"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2022-05-29T12:09:09.168106Z","iopub.status.busy":"2022-05-29T12:09:09.167506Z","iopub.status.idle":"2022-05-29T12:11:06.542621Z","shell.execute_reply":"2022-05-29T12:11:06.541126Z","shell.execute_reply.started":"2022-05-29T12:09:09.168077Z"},"trusted":true},"outputs":[],"source":["model = createDeepLabV3(weights_path= weights_DeepLabV3)\n","accuracy_DL, times_DL, truth_DL, prediction_DL, eval_list_DL = evaluate_and_test_model(model, data_test, display=False)"]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2022-05-29T12:20:44.972132Z","iopub.status.busy":"2022-05-29T12:20:44.971482Z","iopub.status.idle":"2022-05-29T12:21:27.483136Z","shell.execute_reply":"2022-05-29T12:21:27.482378Z","shell.execute_reply.started":"2022-05-29T12:20:44.972063Z"},"trusted":true},"outputs":[],"source":["evaluate(eval_list_DL, truth_DL, prediction_DL)\n","\n","print('AREA EXACTNESS: {:.2f}%'.format(np.mean(accuracy_DL) * 100))\n","print('AVERAGE NECESSARY TIME FOR GENERATING MODEL OUTPUT: {:.3f} second(s)'.format(np.mean(times_DL)))\n","\n","display_confusion_matrix(truth_DL, prediction_DL, save_name='Confusion_Mattrix_DeepLabv3.png')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# Unet - Sigmoid"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2022-05-29T12:11:49.393148Z","iopub.status.busy":"2022-05-29T12:11:49.392715Z","iopub.status.idle":"2022-05-29T12:13:15.611906Z","shell.execute_reply":"2022-05-29T12:13:15.611048Z","shell.execute_reply.started":"2022-05-29T12:11:49.393102Z"},"trusted":true},"outputs":[],"source":["model = createUnet(weights_path= weights_Unet_Sigm, model_type='Sigmoid')\n","accuracy_US, times_US, truth_US, prediction_US, eval_list_US = evaluate_and_test_model(model, data_test, display=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2022-05-29T12:21:27.484972Z","iopub.status.busy":"2022-05-29T12:21:27.484229Z","iopub.status.idle":"2022-05-29T12:22:09.046119Z","shell.execute_reply":"2022-05-29T12:22:09.045406Z","shell.execute_reply.started":"2022-05-29T12:21:27.484932Z"},"trusted":true},"outputs":[],"source":["evaluate(eval_list_US, truth_US, prediction_US)\n","\n","print('AREA EXACTNESS: {:.2f}%'.format(np.mean(accuracy_US) * 100))\n","print('AVERAGE NECESSARY TIME FOR GENERATING MODEL OUTPUT: {:.3f} second(s)'.format(np.mean(times_US)))\n","\n","display_confusion_matrix(truth_US, prediction_US, save_name='Confusion_Mattrix_Unet-Sigmoid.png')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# Unet - ReLU"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2022-05-29T12:13:58.203312Z","iopub.status.busy":"2022-05-29T12:13:58.202464Z","iopub.status.idle":"2022-05-29T12:15:24.406791Z","shell.execute_reply":"2022-05-29T12:15:24.405975Z","shell.execute_reply.started":"2022-05-29T12:13:58.203268Z"},"trusted":true},"outputs":[],"source":["model = createUnet(weights_path= weights_Unet_ReLU, model_type='ReLU')\n","accuracy_UR, times_UR, truth_UR, prediction_UR, eval_list_UR = evaluate_and_test_model(model, data_test, display=False)"]},{"cell_type":"code","execution_count":51,"metadata":{"execution":{"iopub.execute_input":"2022-05-29T12:22:09.048833Z","iopub.status.busy":"2022-05-29T12:22:09.047362Z","iopub.status.idle":"2022-05-29T12:22:50.685751Z","shell.execute_reply":"2022-05-29T12:22:50.684779Z","shell.execute_reply.started":"2022-05-29T12:22:09.048792Z"},"trusted":true},"outputs":[],"source":["evaluate(eval_list_UR, truth_UR, prediction_UR)\n","\n","print('AREA EXACTNESS: {:.2f}%'.format(np.mean(accuracy_UR) * 100))\n","print('AVERAGE NECESSARY TIME FOR GENERATING MODEL OUTPUT: {:.3f} second(s)'.format(np.mean(times_UR)))\n","\n","display_confusion_matrix(truth_UR, prediction_UR, save_name='Confusion_Mattrix_Unet-ReLU.png')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# Image qualitative analysis/comparison"]},{"cell_type":"code","execution_count":54,"metadata":{"execution":{"iopub.execute_input":"2022-05-29T12:25:36.824461Z","iopub.status.busy":"2022-05-29T12:25:36.823875Z","iopub.status.idle":"2022-05-29T12:25:36.843118Z","shell.execute_reply":"2022-05-29T12:25:36.842340Z","shell.execute_reply.started":"2022-05-29T12:25:36.824422Z"},"trusted":true},"outputs":[],"source":["# Marking each class with the color (for example: 0 (granulation) is red color (255, 0, 0) in RGB model)\n","\n","switcher = {\n","        0: [255, 0, 0], # granulation tissue\n","        1: [0, 255, 0], # slough tissue\n","        2: [0, 0, 255], # necrotic tissue\n","        3: [0, 0, 0]    # background\n","    }\n","\n","# Additional function that helps to match an exact class (0-3) with the color (that it is marked in RGB color model)\n","\n","def converting_to_rgb_layers(x):\n","    global switcher\n","    return switcher.get(int(x[0]), \"error\")\n","\n","\n","def convert_into_3d(output_mask):\n","    # create a new 'empty' tensor\n","    zero_tensor = torch.zeros(output_mask.shape[1], output_mask.shape[2], 1)\n","    \n","    # detecting which layer has to be displayed in the picture\n","    dominant_layers = torch.argmax(torch.tensor(output_mask), dim=0).unsqueeze(2)\n","    \n","    # creating a new 3-dimentional tensor\n","    # first dim - dominant layer, other - empty layers\n","    three_dim_tensor = torch.cat((dominant_layers, zero_tensor, zero_tensor), dim=2)\n","    # converting it to numpy array\n","    numpy_array = three_dim_tensor.detach().numpy()\n","        \n","    # changing dominant layers to rgb image\n","    numpy_array = np.apply_along_axis(converting_to_rgb_layers, -1, numpy_array)\n","        \n","    return numpy_array\n","\n","\n","# Creating a function for displaying images with their masks\n","# First 3 images are permanent, the last 3 are changing\n","\n","def display_data(dataset, model=None, n_base=3, n_rand=4):\n","    n = n_base + n_rand\n","    fig, axs = plt.subplots(5,n, figsize=(25,17))\n","    \n","    index_list = []\n","    \n","    for i in range(5):\n","        if(i >= n_base and False):\n","            index=random.randint(n, len(dataset)-1)\n","            print(index)\n","        else:\n","            index = i\n","    #     index= [5,10,14,20,28, 33][i]\n","    \n","        index_list.append(index)\n","\n","        image, mask = dataset[index]\n","        original_image = image.to(device).unsqueeze(0)\n","        original_mask = mask\n","        \n","        image = np.transpose(np.array(image), (1,2,0))\n","        axs[0][i].imshow(image)\n","        if i == 2:\n","            axs[0][i].set_title('Input images')\n","\n","        original_mask = np.transpose(np.array(original_mask), (1,2,0))\n","        axs[1][i].imshow(original_mask)\n","        if i == 2:\n","            axs[1][i].set_title('Original masks')\n","            \n","     \n","\n","    for a in range(3):\n","        if a==0:\n","            model = createDeepLabV3(weights_path= weights_DeepLabV3)\n","            title = 'DeepLabV3'\n","        elif a==1:\n","            model = createUnet(weights_path= weights_Unet_Sigm, model_type='Sigmoid')\n","            title = 'Unet - Sigmoid'\n","        else:\n","            model = createUnet(weights_path= weights_Unet_ReLU, model_type='ReLU')\n","            title = 'Unet - ReLU'\n","        \n","        axs[2+a][2].set_title(title)\n","\n","        for i, index in enumerate(index_list):\n","            image, mask = dataset[index]\n","            original_image = image.to(device).unsqueeze(0)\n","            original_mask = mask\n","            model.eval()\n","\n","            with torch.no_grad():            \n","                output_mask = model(original_image)\n","                if isinstance(output_mask, dict):\n","                    output_mask = output_mask['out']\n","                output_mask  = output_mask.squeeze(0).cpu()\n","                output_mask = convert_into_3d(output_mask) # decoding the output\n","                axs[2+a][i].imshow(output_mask)\n","            \n","    for a in range(5):\n","        for b in range(n_base + n_rand):\n","            axs[a][b].axis('off')\n","    \n","\n","#     plt.subplots_adjust(left=0.1, bottom=0.1, right=0.7, top=0.9, wspace=0.3, hspace=0.3) \n","    plt.savefig('image_comparison.png')\n","    plt.show() "]},{"cell_type":"code","execution_count":55,"metadata":{"execution":{"iopub.execute_input":"2022-05-29T12:25:39.634156Z","iopub.status.busy":"2022-05-29T12:25:39.633395Z","iopub.status.idle":"2022-05-29T12:26:03.437248Z","shell.execute_reply":"2022-05-29T12:26:03.436474Z","shell.execute_reply.started":"2022-05-29T12:25:39.634115Z"},"trusted":true},"outputs":[],"source":["display_data(data_test)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
